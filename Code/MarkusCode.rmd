---
title: "MarkusCode"
author: "Markus Gerholm"
date: "2025-12-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Fixing directory for markdown file

```{r wd-check, echo=TRUE}
getwd()
list.files()
file.exists("Data/sweden_returns_merged.csv")
```

## Loading data

```{r}
library(dplyr)

df <- read.csv(file.path("..", "Data", "sweden_returns_merged.csv"))
```

## Choosing time period: 2019 - 2021

```{r}
df_2019_2021 <- df %>%
  mutate(X = as.Date(X)) %>%   # if this gives NA, see note below
  filter(X >= as.Date("2019-01-01"),
         X <= as.Date("2021-12-31"))

```

## Checking for missing values

```{r}
na_counts <- sort(colSums(is.na(df_2019_2021)), decreasing = TRUE)
na_counts[na_counts > 0]
```

## Removing all assets that have number of NA > 1 for time period

```{r}
na_counts <- colSums(is.na(df_2019_2021))
keep_cols <- names(na_counts)[na_counts <= 1]
keep_cols <- union("X", setdiff(keep_cols, "X"))  # ensure X stays first/kept
df_clean <- df_2019_2021 %>% select(all_of(keep_cols))
```

## Interpolating/Extrapolating the two missing values for the two assets with only one value missing

```{r}
df_clean$INCOAX.ST[1] <- 0 # assign 0 as if stock did not move
df_clean$RO.ST[219] <- 0 # assign 0 as if stock did not move
df_clean <- df_clean[, setdiff(names(df_clean), "BOTX.ST")] # removing this stock since it was only zeros
```

## Finding groups through Hierarchical clustering

```{r}
R <- as.matrix(df_clean[ , setdiff(names(df_clean), "X")])

# if you still have any NA, decide how to handle:
colSums(is.na(R))[colSums(is.na(R)) > 0]

```

```{r}
library(corpcor)

C <- corpcor::cor.shrink(R)  # shrinkage correlation

```

```{r}
D  <- as.dist(sqrt((1 - C) / 2))
hc <- hclust(D, method = "ward.D2")
plot(hc, labels = FALSE, main = "Correlation-based clusters (Ward)")

```

## Checking different values of K

```{r}
Ks <- c(6, 8, 10, 12, 15, 20)

op <- par(mfrow = c(ceiling(length(Ks)/2), 2), mar = c(3, 2, 3, 1))
for (k in Ks) {
  plot(hc, labels = FALSE, main = paste("Cut at K =", k))
  rect.hclust(hc, k = k, border = 2:(k+1))
}
par(op)

```


## Choosing number of clusters and creating table with number of stocks within each cluster

```{r}
k <- 15
clusters <- cutree(hc, k = k)
table(clusters)

```

## Option A: averaging standardized stocks within each cluster

```{r}
assets <- setdiff(names(df_2019_2021), "X")
R <- as.matrix(df_2019_2021[, assets])

# clusters: named integer vector, names(clusters) == assets
# e.g. clusters <- cutree(hc_assets, k = 12); names(clusters) <- assets

Rz <- scale(R)  # center/scale each asset

cluster_ids <- sort(unique(clusters))
Y <- sapply(cluster_ids, function(k) rowMeans(Rz[, clusters == k, drop = FALSE]))

colnames(Y) <- paste0("cluster_", cluster_ids)
Y <- as.data.frame(Y)
Y$X <- df_2019_2021$X

```

## Option B: First principal component score within each cluster

We start off by trying to standardize with GARCH(1,1) and t-distributed innovations

```{r}
library(rugarch) # For GARCH(1,1)

# GARCH(1,1) with Student-t innovations
spec <- ugarchspec(
  variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
  mean.model     = list(armaOrder = c(0, 0), include.mean = TRUE),
  distribution.model = "std"
)

pc1_factor_garch <- function(M) {
  # M: T x n matrix for ONE cluster (no NAs, no constant columns)
  Z <- sapply(seq_len(ncol(M)), function(j) {
    fit <- ugarchfit(spec = spec, data = as.numeric(M[, j]), solver = "hybrid")
    as.numeric(residuals(fit, standardize = TRUE))
  })

  p <- prcomp(Z, center = TRUE, scale. = FALSE)
  as.numeric(scale(p$x[, 1]))  # PC1 time series (standardized)
}

cluster_ids <- sort(unique(clusters))
Y_garch <- sapply(cluster_ids, function(k) pc1_factor_garch(R[, clusters == k, drop = FALSE]))

colnames(Y_garch) <- paste0("cluster_", cluster_ids)
Y_garch <- as.data.frame(Y_garch)
Y_garch$X <- df_clean$X

```

## Checking normality assumption

```{r}
qqnorm(Y_garch$cluster_1)
qqline(Y_garch$cluster_1, col = "red")
```

## Standardization through stochastic volatility and clustering through first principal component

```{r}
library(stochvol) # For Stochastic Volatility
library(xts)

# --- SV-standardize one xts column (your Taylor-style SV) ---
sv_standardize_xts <- function(r_xts, draws = 2000, burnin = 500) {
  r_xts <- na.omit(r_xts)
  y <- as.numeric(r_xts)
  y <- y - mean(y)

  fit <- svsample(y, draws = draws, burnin = burnin, quiet = TRUE)
  sig_hat <- colMeans(vola(fit))   # posterior mean sigma_t

  z <- y / sig_hat
  xts(z, order.by = index(r_xts))
}

# --- PC1 factor from SV-standardized residuals for one cluster ---
pc1_factor_sv <- function(M_xts, draws = 2000, burnin = 500) {
  Z_xts <- do.call(merge, lapply(colnames(M_xts), function(nm) {
    z <- sv_standardize_xts(M_xts[, nm, drop = FALSE], draws = draws, burnin = burnin)
    colnames(z) <- nm
    z
  }))

  Z <- coredata(Z_xts)
  p <- prcomp(Z, center = TRUE, scale. = FALSE)
  as.numeric(scale(p$x[, 1]))  # PC1 time series
}

assets <- setdiff(names(df_clean), "X")
R_xts <- xts(as.matrix(df_clean[, assets]), order.by = df_clean$X)

cluster_ids <- sort(unique(clusters))

Y_sv <- sapply(cluster_ids, function(k) {
  nm <- names(clusters)[clusters == k]
  pc1_factor_sv(R_xts[, nm, drop = FALSE], draws = 2000, burnin = 500)
})
colnames(Y_sv) <- paste0("cluster_", cluster_ids)

Y_sv <- as.data.frame(Y_sv)
Y_sv$X <- df_clean$X
```

## No clustering only standardization through stochastic volatility on each asset

```{r}
library(stochvol)
library(xts)

sv_standardize_xts <- function(r_xts, draws = 2000, burnin = 500) {
  y <- as.numeric(r_xts)
  y <- y - mean(y)

  fit <- svsample(y, draws = draws, burnin = burnin, quiet = TRUE)
  sig_hat <- colMeans(vola(fit))
  z <- y / sig_hat
  xts(z, order.by = index(r_xts))
}

assets <- setdiff(names(df_clean), "X")
R_xts <- xts(as.matrix(df_clean[, assets]), order.by = df_clean$X)

# SV-standardize EVERY asset column
Z_xts <- do.call(merge, lapply(colnames(R_xts), function(nm) {
  z <- sv_standardize_xts(R_xts[, nm, drop = FALSE], draws = 2000, burnin = 500)
  colnames(z) <- nm
  z
}))

# convert to a data.frame with dates
Z_df <- data.frame(X = as.Date(index(Z_xts)), coredata(Z_xts))

```


```{r}
qqnorm(Z_df$AZN.ST)
qqline(Z_df$AZN.ST, col = "red")
```

```{r}
Z_0 <- coredata(Z_xts)
Z_0 <- scale(Z_0, center = TRUE, scale = FALSE)
```

## Regular GLASSO on standardized assets

```{r}
library(huge)
## Testing GLASSO
fit <- huge(Z_0, method = "glasso")

```

```{r}
plot(fit)
```
```{r}
library(igraph)
adj <- fit$path[3]
adj <- adj[[1]]
g <- graph_from_adjacency_matrix(adj, mode = "undirected", diag = FALSE)
```

```{r}
set.seed(1)
lay <- layout_with_fr(g)

# remove margins so the graph uses the full device
par(mar = c(0, 0, 0, 0))

# zoom factor: < 1 zooms in, > 1 zooms out
zoom <- 1

xlim <- mean(range(lay[,1])) + (range(lay[,1]) - mean(range(lay[,1]))) * zoom
ylim <- mean(range(lay[,2])) + (range(lay[,2]) - mean(range(lay[,2]))) * zoom

plot(g,
     layout = lay,
     vertex.size = 5,
     edge.width  = 1,
     vertex.label = NA,
     rescale = FALSE,
     xlim = xlim,
     ylim = ylim)
```

## Fitting GLASSO on first and second half of the time period

```{r}
Z_first <- Z_0[1:253, , drop = FALSE]
Z_last <- Z_0[254:506, , drop = FALSE]

fit1 <- huge(Z_first, method = "glasso", lambda = 0.35)
fit2 <- huge(Z_last, method = "glasso", lambda= 0.35)

adj1 <- fit1$path[1]
adj1 <- adj1[[1]]

adj2 <- fit2$path[1]
adj2 <- adj2[[1]]

g1 <- graph_from_adjacency_matrix(adj1, mode = "undirected", diag = FALSE)
g2 <- graph_from_adjacency_matrix(adj2, mode = "undirected", diag = FALSE)

```

```{r}
set.seed(1)
#lay <- layout_with_fr(g1)
lay <- layout_on_grid(g1)

# remove margins so the graph uses the full device
par(mar = c(0, 0, 0, 0))

# zoom factor: < 1 zooms in, > 1 zooms out
zoom <- 1

xlim <- mean(range(lay[,1])) + (range(lay[,1]) - mean(range(lay[,1]))) * zoom
ylim <- mean(range(lay[,2])) + (range(lay[,2]) - mean(range(lay[,2]))) * zoom

plot(g1,
     layout = lay,
     vertex.size = 5,
     edge.width  = 1,
     vertex.label = NA,
     rescale = FALSE,
     xlim = xlim,
     ylim = ylim)
```

```{r}
set.seed(1)
#lay <- layout_with_fr(g2)
lay <- layout_on_grid(g2)

# remove margins so the graph uses the full device
par(mar = c(0, 0, 0, 0))

# zoom factor: < 1 zooms in, > 1 zooms out
zoom <- 1

xlim <- mean(range(lay[,1])) + (range(lay[,1]) - mean(range(lay[,1]))) * zoom
ylim <- mean(range(lay[,2])) + (range(lay[,2]) - mean(range(lay[,2]))) * zoom

plot(g2,
     layout = lay,
     vertex.size = 5,
     edge.width  = 1,
     vertex.label = NA,
     rescale = FALSE,
     xlim = xlim,
     ylim = ylim)
```
We see that when fixing lambda to the lowest value from the fit of the complete time period, the number of edges increase in the second half of the time period

## Time-varying GLASSo

```{r}
library(tvsfglasso)
Y <- t(Z_0) # row = assets, cols = time points
N = ncol(Y) # number of time points
pos <- 1:N # length of data as sequence
res <- tvglasso(Y = Y, N = N, pos = pos, rep = FALSE, lambda = 0.5) # fitting tvglasso model
```

```{r}
A1 <- res$icov[[1]]
AN <- res$icov[[N]]

A1[A1 != 0] <- 1
AN[AN != 0] <- 1

g1_tv <- graph_from_adjacency_matrix(A1, mode = "undirected", diag = FALSE) # precision matrix at time 1
gN_tv <- graph_from_adjacency_matrix(AN, mode = "undirected", diag = FALSE) # precision matrix at time N
```


```{r}
set.seed(1)
lay <- layout_with_fr(g1_tv)

# remove margins so the graph uses the full device
par(mar = c(0, 0, 0, 0))

# zoom factor: < 1 zooms in, > 1 zooms out
zoom <- 1

xlim <- mean(range(lay[,1])) + (range(lay[,1]) - mean(range(lay[,1]))) * zoom
ylim <- mean(range(lay[,2])) + (range(lay[,2]) - mean(range(lay[,2]))) * zoom

plot(g1_tv,
     layout = lay,
     vertex.size = 5,
     edge.width  = 1,
     vertex.label = NA,
     rescale = FALSE,
     xlim = xlim,
     ylim = ylim)
```

```{r}
set.seed(1)
lay <- layout_with_fr(gN_tv)

# remove margins so the graph uses the full device
par(mar = c(0, 0, 0, 0))

# zoom factor: < 1 zooms in, > 1 zooms out
zoom <- 1

xlim <- mean(range(lay[,1])) + (range(lay[,1]) - mean(range(lay[,1]))) * zoom
ylim <- mean(range(lay[,2])) + (range(lay[,2]) - mean(range(lay[,2]))) * zoom

plot(gN_tv,
     layout = lay,
     vertex.size = 5,
     edge.width  = 1,
     vertex.label = NA,
     rescale = FALSE,
     xlim = xlim,
     ylim = ylim)

```

## Creating GIF of network for every time step

```{r}
library(igraph)
library(gifski)

# --- assumes `res` already exists ---
N   <- length(res$icov)
pos <- seq_len(N)   # or your existing pos if you have one

# Convert each precision matrix to a binary adjacency matrix
# (If you prefer strict nonzero like your example, keep M != 0.
#  If you want to avoid tiny numerical noise, use abs(M) > tol instead.)
tol <- 1e-10
A_list <- lapply(res$icov, function(M) {
  #A <- (abs(M) > tol) * 1   # <- change to (M != 0) * 1 if you want exact nonzero
  A <- (M != 0) * 1
  diag(A) <- 0
  A
})

# Build graphs for each time point
G_list <- lapply(A_list, function(A)
  graph_from_adjacency_matrix(A, mode = "undirected", diag = FALSE)
)

# Stable layout from t = 1 (your choice)
set.seed(1)
#lay <- layout_with_fr(G_list[[1]])
lay <- layout_on_grid(G_list[[1]])

# Your zoom + fixed view window (so frames don't jump)
zoom <- 1
xlim <- mean(range(lay[,1])) + (range(lay[,1]) - mean(range(lay[,1]))) * zoom
ylim <- mean(range(lay[,2])) + (range(lay[,2]) - mean(range(lay[,2]))) * zoom

# Save animation as GIF
save_gif({
  op <- par(mar = c(0, 0, 0, 0))  # remove margins
  on.exit(par(op), add = TRUE)

  for (t in seq_along(G_list)) {
    plot(
      G_list[[t]],
      layout = lay,
      vertex.size  = 5,
      edge.width   = 1,
      vertex.label = NA,
      rescale = FALSE,
      xlim = xlim,
      ylim = ylim
    )

    # Optional time label without adding margins:
     usr <- par("usr")
     text(usr[1], usr[4], labels = paste0("t = ", pos[t]),
          adj = c(0, 1), xpd = NA, cex = 1)
  }
}, gif_file = "tvglasso_evolution4.gif", width = 800, height = 800, delay = 0.08)

```




